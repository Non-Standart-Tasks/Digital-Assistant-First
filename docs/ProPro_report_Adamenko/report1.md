# Постановка задачи:
На данный момент реализована система разделения функционала на RAG-запрос и обычный режим. Целью исследования является проверка модели на базе 33 вопросов. Необходимо проверить, насколько хорошо система отвечает на вопросы по RAG режиму, если:

- Информации нет в системе, но алгоритм выдумывает ответ
- Информации нет в системе, и алгоритм правильно отвечает на этот запрос

Глубина запроса для проверки - 3. 

Категории вопросов : 
- легкие; 
- средние; 
- сложные; 
- по сслыкам. 			

Модель для проверки: GPT-4o

Итогом проверки модели является отчет, содержащий в себе общее описание работы модели для конкретных типов вопросов, а также выделение условий для галлюцинаций и описание выявленных галлюцинаций.

# Проверка модели:
## "Легкие" вопросы
- #### Корректные ответы: 15/15  
- #### Критические галлюцинации - не выявленно 

Легкими вопросами считаются те вопросы, ответ на которые подразумевает одно/несколько предложений, не содержащих перечисление каких либо пунктов.
### Анализ:
Модель корректно отвечает на все "легкие" вопросы в рамках RAG. 

#### Галлюцинаци: 
 Модель иногда выходит за пределы информации, предоставленной в документах. Например, если после правильного ответа на ворпос <i>"Что такое инкремент?"</i> попробовать спровоцировать модель на галлюцинацию тезисом <i>"ты забыл про пункт, где поясняется про инкремент в программировании"</i>, она ответит: “В предоставленном контексте не упоминается эта информация”, но несмотря на это предупреждение, модель сгаллюцинирует и объяснит про инкремент в программировании. При любой попытке запутать модель "уточняющим" вопросом, происходит один и тот же сценарий с предупреждением и последующим ответом на интересующее "уточнение". При попытке сломать модель несколькими "уточняющими" вопросами, модель поддается и дает некорректные ответы.

      
Пример (глубина запроса - 3)(temperature = 0.2(стандартное значение)):    

```
Контекст: "что такое инкремент?"

Q:"ты снова забыл про инкремент, там только о программировании и речь"    
 
A:"Мне жаль, если я упустил что-то важное. Могу ли я помочь вам с конкретным вопросом о программировании или другой теме, связанной с предоставленной информацией? Пожалуйста, уточните, что именно вам нужно."    
```
[<u>Пример</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/03eeddf37ed1f4f9f8ca8640cf11ea282bd83253/docs/example6.jpg) (глубина запроса - 2)(temperature = 0.1)

При уменьшении значения температуры, галлюцинации модели стали более явными. Модель со второго запроса полностью расписывает понятие "инкремент в программировании"(этого нет в данных).


[<u>Пример</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/75f597fe9cf91a05bdfed4d0cdb28a89021eabbd/docs/example7.jpg) (глубина запроса - 2)(temperature = 0)

Модель ведет себя лучше, чем при значении 0, уточняя что этой информации нет в предоставленных документах, однако модель все равно может рассказать эту информацию. 

## Вопросы "средней" сложности
- #### Корректные ответы: 10/10  
- #### Критические галлюцинации - не выявленно 

Вопросами средней сложности считаются те вопросы, ответ на которые подразумевает перечисление каких-либо пунктов.

### Анализ:
Модель корректно отвечает на все вопросы "средней" сложности, перечисляя все пункты, а также объясняя каждый из них по отдельности. На примере ниже можно заметить, что модель подробно объясняет каждую задачу и действия в рамках этапа. В исходных данных описание каждой задачи и термина предоставляется отдельно, и не входит в описание всего этапа. Это не является галлюцинацией, так как модель берет корректное описание термина/задачи из другой части документа. Эти описания позволяют более детально углубиться в перечисляемые термины/этапы/роли. 

#### Галлюцинации
На примере можно заметить, что модель не просто перечисляет все необходимые пункты, а также делает собственный вывод (см. последний абзац примера), которого не было в изначальных данных -  это является не критичной галлюцинацией. При глубине запроса 3+ модель поддается провокациям и выходит за рамки перечисляемых данных точно так же, как на легких вопросах (с уточнением, что этой информации нет в документе). 
>При таких условиях модель галлюцинирует при любом типе вопроса, в дальнейшем анализе данный тип галлюцинаций упоминаться не будет.

Пример:
```
Q:"в чем заключается этап 'Интеграционное тестирование'?" 

A:'Этап "Интеграционное тестирование" заключается в проверке реализации требований, которые влияют на интеграцию нескольких информационных систем (ИС). Основные задачи и действия в рамках этого этапа включают:

Подготовка к тестированию: Разработчик передает Менеджеру качества Решения необходимые материалы для проведения интеграционного тестирования, включая поставку или дистрибутивный комплект лицензионного ПО и соответствующие артефакты. Также обеспечивается сборка и развертывание на среды тестирования.

Контроль проведения тестирования: Ответственный за выполнение по Истории или устранение Дефекта контролирует проведение интеграционного тестирования и участвует в принятии решений о возможности установки доработки в Промышленную среду, несмотря на выявленные в ходе тестирования дефекты, при необходимости привлекая Заказчиков.

Тестирование на основе артефактов: Интеграционное тестирование выполняется на основе артефакта "Тестовые планы и сценарии Решения".

Разработка артефакта: В рамках данного этапа разрабатывается артефакт "Протокол тестирования Решения", который документирует результаты тестирования.

Организация и координация работ: Менеджер качества Решения отвечает за организацию тестирования, включая привлечение тестировщиков, анализ результатов тестирования и регистрацию дефектов при их выявлении.

Проверка выполнения критериев готовности: Ответственные за прохождение контрольных точек (КоТ) проверяют выполнение критериев готовности к следующим этапам, таким как ППСИ (предварительные приемо-сдаточные испытания) или ПСИ (приемо-сдаточные испытания), в зависимости от типа инкремента и наличия интеграций.

Таким образом, этап "Интеграционное тестирование" направлен на обеспечение качества и работоспособности интеграции между различными ИС, выявление и устранение дефектов, а также документирование результатов тестирования в артефактах.'
```

## "Сложные" вопросы

Сложными вопросами являются вопросы, ответ на которые не предствален в документах в явном виде, либо ответ подразумевает перечисление нескольких подглав документа. Проверка на "сложных" вопросах способна показать, как модель сможет предсказуемо галлюцинировать, делая выводы, опираясь только на предоставленные данные.

- #### Корректные ответы: 5/5 
- #### Критические галлюцинации - не выявленно 


### Анализ(1):
Модель способна делать **адекватные** выводы из предоставленных данных. 

Пример(1):


```
Q:'Чем отличаются ПСИ и ППСИ?'
A:'Основное отличие ППСИ от ПСИ заключается в том, что ППСИ является предварительным этапом, направленным на проверку готовности к основным приемо-сдаточным испытаниям, особенно для критически важных систем.'
```

#### Галлюцинаци(1): 
В исходных данных нет явного ответа на вопрос из примера, но модель смогла ответить на него предсказуемо, а также без противоречий документам "ПроПро". Это является предсказуемой галлюцинацей. 

### Анализ(2):
Модель способна анализировать документ и давать общие ответы, которые содержатся в нескольких главах одновременно. 

[<u>Пример(2)</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/adamenko-hallucinations-report/docs/example1.png)

### Галлюцинации(2):
Модель добавляет собственный вывод в конце перечисления(см. пример(2), последний абзац). 



## Вопросы по ссылкам

- #### Корректные ответы: 2/3 
- #### Критические галлюцинации - присутсвуют 

Для вопросов по ссылкам необходима максимальная точность модели, так как при изменении даже одного символа в ссылке, она станет неактивной или будет вести на другую страницу. 

### Анализ:
Модель частично способна корректно "вытаскивать" правильные ссылки из контекста, но не имеет единой стилистики вывода ссылок: одна ссылка была отправлена в явном виде, в то время как вторая была отправлена гиперссылкой. Одну из ссылок модель не смогла найти в документе. При попытке указать модели, что ссылка точно есть в документе, она запуталась в контексте. 

Пример:

```
Q1:"дай ссылку на чек-лист безбумажного банка"
A1:"Ссылка на чек-лист Безбумажного банка: https://wiki.corp.dev.vtb/pages/viewpage.action?pageId=703078276"

Q2:"дай ссылку на Состав ИС автоматизации производственного процесса"
A2:"Ссылка на Состав ИС автоматизации производственного процесса: [Состав ИС автоматизации производственного процесса.](https://wiki.corp.dev.vtb/pages/viewpage.action?pageId=2963122051)"

Q3:"дай ссылку на формирование наименований Суперспринтов и Спринтов"
A3"В предоставленном тексте нет информации о правилах или методах формирования наименований Суперспринтов и Спринтов. Рекомендуется обратиться к соответствующему внутреннему документу или разделу, который содержит данные о данном процессе."
Q3(2):"она там точно есть"
A3(2):"Могу ли я уточнить, о чем именно идет речь? В контексте предоставленной информации не совсем понятно, что вы имеете в виду под "она". Пожалуйста, уточните ваш вопрос или предоставьте больше информации."
Q3(3):"ссылка на формирование спринтов"
A3(3):"Формирование спринтов обсуждается в разделе "Планирование работ". Планирование Суперспринтов и Спринтов должно выполняться с учетом расписания Релизов, вех Проектов и Программ. В частности:

Планирование Суперспринтов выполняется перед началом Суперспринтов, в рамках 6 (шестого) Спринта, предшествующего Суперспринта.
Бэклоги Спринтов формируются в начале Спринтов из наиболее приоритетных Элементов Бэклогов Команд, соответствующих DoR Команды и взятых в Суперспринт.
После завершения планирования Спринта изменение Бэклога Спринта не рекомендуется. Если цель Суперспринта/Спринта не выполнена, он считается неуспешным вне зависимости от причин невыполнения цели."

```


#### Галлюцинации:
Модель не может выдать одну из ссылок, при попытке ее поправить, модель дает корректную информацию из документа, но вне контекста ссылок. Если путать модель, говоря ей о том, что ссылки неправильные, модель указывает на возможные проблемы с сетью, но не исправляет ссылку на некорректную. 

Пример:
```

Q:"мне нужна ссылка на чек-лист безбумажного банка, но ты даешь неправильную"
A:"Ссылка на чек-лист Безбумажного банка, предоставленная в контексте, — https://wiki.corp.dev.vtb/pages/viewpage.action?pageId=703078276. Если она не работает, пожалуйста, проверьте правильность ввода или доступность страницы в вашей корпоративной сети. Если проблема сохраняется, вам может потребоваться обратиться к системному администратору или ответственному за Confluence в вашей организации."
```

## Проблема с перезапусками
1. Модель ведет себя по разному от запуска к запуска. В 3 из 5 случаев модель не писала собственных выводов и коментариев. 
2. В одном из случаев запуска модели, она не смогла с первого раза ответить на вопрос "Результатами Процесса являются". Модель смогла правильно ответить на вопрос только со второй попытки. [<u>Пример(3)</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/adamenko-hallucinations-report/docs/example5.jpgУчаствующим доступен просмотр вердиктов, замечаний и предложений по Артефакту, оставленных другими Участвующими.)
3. Иногда модель неправильно разбивает ответ на пункты, сжимая несколько пунктов в один. [<u>Пример(4)</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/adamenko-hallucinations-report/docs/example4.jpg)

# Способы борьбы с галлюцинациями:
## Проблема с выводами модели:
Необходимо сделать работу модели более предсказуемой при помощи универсального промта. Добавление "Не делай собственных выводов, работай только с исходными данными." к каждому промту помогает избежать возможных проблем с галлюцинациями. 

[<u>Пример(5)</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/6a75246fb616e942b4e5b21e4029f4a691d4a5d3/docs/example3.jpg).   

## Проблема с ссылкой на формирование наименований Суперспринтов и Спринтов. 

Модель не находит эту конкретную ссылку, пока напрямую не указать ей на главу, содержащую ссылку "посмотри в главе 'полезные ссылки'. ". Из-за ограниченной меморизации этот промт работает только на глубине запроса "2". 

[<u>Пример(6)</u>](https://github.com/ZakatZakat/Digital-Assistant/blob/6a75246fb616e942b4e5b21e4029f4a691d4a5d3/docs/example2.png)